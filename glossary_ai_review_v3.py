# -*- coding: utf-8 -*-
import pandas as pd
import openai
import re
import os
import time
import json
import sys
import concurrent.futures
from typing import Dict, Any, Optional, List
import importlib.util
import threading
import random

# --- ä¾èµ–åº“æ£€æŸ¥ (Dependency Check) ---
def check_dependencies():
    """æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„åº“æ˜¯å¦å·²å®‰è£…ã€‚"""
    required_dependencies = {
        'pandas': 'pandas',
        'openai': 'openai',
        'openpyxl': 'openpyxl',
        'tqdm': 'tqdm',
        'xlsxwriter': 'XlsxWriter'
    }
    missing_dependencies = []

    for package_name in required_dependencies.keys():
        spec = importlib.util.find_spec(package_name)
        if spec is None:
            missing_dependencies.append(required_dependencies[package_name])

    if missing_dependencies:
        print("é”™è¯¯ï¼šè„šæœ¬è¿è¡Œç¼ºå°‘å¿…è¦çš„ Python åº“ã€‚")
        print(f"ç¼ºå¤±çš„åº“: {', '.join(missing_dependencies)}")
        print("\nè¯·å¤åˆ¶å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…å®ƒä»¬:")
        print(f"pip install {' '.join(missing_dependencies)}")
        sys.exit(1)
    
    global tqdm
    try:
        from tqdm import tqdm
    except ImportError:
        def tqdm(iterable, *args, **kwargs):
            return iterable

# --- å…¨å±€é…ç½®å˜é‡ ---
CONFIG = {}
rate_limit_pause_event = threading.Event()

# --- æ–‡ä»¶è·¯å¾„å˜é‡ ---
GLOSSARY_FILE_PATH = None 
REFERENCE_FILE_PATH = None
FINAL_GLOSSARY_FILENAME = 'glossary_output.xlsx'
MODIFICATION_LOG_FILENAME = 'modified.xlsx'
ERROR_LOG_FILENAME = 'error_log.txt'
FINAL_GLOSSARY_OUTPUT_PATH = ''
MODIFICATION_LOG_OUTPUT_PATH = ''
ERROR_LOG_OUTPUT_PATH = ''


# --- AI Prompt æ¨¡æ¿ (V3) ---
BATCH_REVIEW_PROMPT_TEMPLATE = """
è§’è‰²ï¼šä¸“ä¸šå°è¯´ç¿»è¯‘å®¶ (V3 - æ‰¹å¤„ç†æ¨¡å¼)

èº«ä»½ä¸ä½¿å‘½:
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„éŸ©ä¸­ç¿»è¯‘å¤å®¡ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ¥æ”¶ä¸€æ‰¹æœ¯è¯­ï¼Œå¹¶å¯¹å…¶ä¸­çš„æ¯ä¸€æ¡è¿›è¡Œç‹¬ç«‹çš„ã€ç²¾ç¡®çš„å®¡æŸ¥ã€‚

æ ¸å¿ƒè¡Œä¸ºå‡†åˆ™:
- ç»å¯¹å¿ äºâ€œå°è¯´èƒŒæ™¯è®¾å®šâ€å’Œâ€œæœ¯è¯­æ‰€åœ¨åŸæ–‡å‚è€ƒâ€ï¼Œè¿™æ˜¯ä½ åˆ¤æ–­çš„æœ€é«˜ä¾æ®ã€‚
- å¯¹äºä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€ç»„ç»‡ç­‰ï¼‰ï¼Œä½ çš„é¦–è¦ä»»åŠ¡æ˜¯ç¡®ä¿å…¶â€œä¸€è‡´æ€§â€ï¼Œåœ¨æ²¡æœ‰æ˜æ˜¾é”™è¯¯çš„æƒ…å†µä¸‹ä¸è½»æ˜“ä¿®æ”¹ã€‚
- å¯¹äºæ™®é€šè¯æ±‡ï¼Œä½ çš„ä»»åŠ¡æ˜¯â€œç²¾ç®€â€ï¼Œå¤§èƒ†åœ°åˆ é™¤ä¸å¿…è¦çš„é€šç”¨è¯ã€åŠ¨è¯å’Œæè¿°æ€§çŸ­è¯­ï¼Œåªä¿ç•™æ ¸å¿ƒåè¯ã€‚

å°è¯´èƒŒæ™¯è®¾å®š:
{novel_background}

ä»»åŠ¡ï¼šæ‰¹é‡æœ¯è¯­å®¡æŸ¥
è¯·æ ¹æ®â€œå°è¯´èƒŒæ™¯è®¾å®šâ€å’Œæ¯ä¸ªæœ¯è¯­å„è‡ªçš„â€œæœ¯è¯­æ‰€åœ¨åŸæ–‡å‚è€ƒâ€ï¼Œç‹¬ç«‹åˆ¤æ–­åˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªæœ¯è¯­æ˜¯å¦æœ‰ç¿»è¯‘é—®é¢˜ã€‚
å®¡æŸ¥æ ‡å‡†å¦‚ä¸‹ï¼š
1. æ˜¯å¦ä¸ºå¤šä¹‰è¯ï¼Ÿï¼ˆå»ºè®®åˆ é™¤ï¼‰
2. ç¿»è¯‘æ˜¯å¦å‡†ç¡®ï¼Ÿ
3. æ˜¯å¦ä¸ºé€šç”¨è¯ï¼ˆå³æ²¡æœ‰æ­§ä¹‰çš„æ—¥å¸¸è¯æ±‡ï¼Œå¦‚â€œåºŠå•â€ã€â€œæ°´å£¶â€ï¼‰ï¼Ÿï¼ˆå»ºè®®åˆ é™¤ï¼‰
4. æ˜¯å¦ä¸ºå½¢å®¹è¯ã€åŠ¨è¯æˆ–æè¿°æ€§çŸ­è¯­ï¼Ÿï¼ˆå»ºè®®åˆ é™¤ï¼‰
5. å¦‚æœæ˜¯è§’è‰²æœ¯è¯­ï¼Œäººåã€æ€§åˆ«ã€ä¸€è‡´æ€§æ˜¯å¦æ­£ç¡®ï¼Ÿå¦‚æœä¸æ˜¯è§’è‰²ï¼Œæ˜¯å¦åº”åˆ é™¤ï¼Ÿ

è¯·ä¸¥æ ¼æŒ‰ç…§æˆ‘ç»™å‡ºçš„ JSON æ ¼å¼è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æœ¯è¯­å®¡æŸ¥ç»“æœçš„ JSON åˆ—è¡¨ã€‚åˆ—è¡¨çš„é¡ºåºå¿…é¡»ä¸è¾“å…¥åˆ—è¡¨çš„é¡ºåºå®Œå…¨ä¸€è‡´ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå¤„ç†èŒƒä¾‹ï¼š
---
[èŒƒä¾‹è¾“å…¥]
[
  {{ "korean_term": "ì¹¨ëŒ€ ì‹œíŠ¸", "chinese_translation": "åºŠå•", "is_character": false, "context": "ê·¸ëŠ” ì¹¨ëŒ€ ì‹œíŠ¸ë¥¼ ê°ˆì•˜ë‹¤. (ä»–æ¢äº†åºŠå•ã€‚)" }},
  {{ "korean_term": "í˜„ì¬ì›…", "chinese_translation": "ç„åœ¨é›„", "is_character": true, "context": "í˜„ì¬ì›…ì€ ë§í–ˆë‹¤. (ç„åœ¨é›„è¯´é“ã€‚)" }}
]

[èŒƒä¾‹è¾“å‡º]
[
  {{
    "korean_term": "ì¹¨ëŒ€ ì‹œíŠ¸",
    "original_translation": "åºŠå•",
    "recommended_translation": "åºŠå•",
    "should_delete": true,
    "deletion_reason": "é€šç”¨è¯",
    "judgment_emoji": "ğŸ—‘ï¸",
    "justification": "è¯¥æœ¯è¯­ä¸ºé€šç”¨è¯ï¼ˆæ—¥å¸¸è¯æ±‡ï¼‰ï¼Œæ— ç‰¹æ®Šå«ä¹‰ï¼Œå»ºè®®åœ¨æœ€ç»ˆæœ¯è¯­è¡¨ä¸­åˆ é™¤ã€‚"
  }},
  {{
    "korean_term": "í˜„ì¬ì›…",
    "original_translation": "ç„åœ¨é›„",
    "recommended_translation": "ç„åœ¨é›„",
    "should_delete": false,
    "deletion_reason": null,
    "judgment_emoji": "âœ…",
    "justification": "è§’è‰²åç¿»è¯‘å‡†ç¡®ï¼Œä¸èƒŒæ™¯ä¸€è‡´ã€‚"
  }}
]
---

ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹æœ¯è¯­åˆ—è¡¨ï¼š
{batch_json}

è¾“å‡ºæ ¼å¼ (Output Format):
[
  {{
    "korean_term": "[æœ¯è¯­åŸæ–‡]",
    "original_translation": "[åŸå§‹è¯‘æ–‡]",
    "recommended_translation": "[ä½ çš„é¦–é€‰å»ºè®®]",
    "should_delete": "[true/false]",
    "deletion_reason": "[é€šç”¨è¯/åŠ¨è¯/å½¢å®¹è¯/æè¿°æ€§çŸ­è¯­/éè§’è‰²/å…¶ä»–/null]",
    "judgment_emoji": "[âœ…/âš ï¸/âŒ/ğŸ—‘ï¸]",
    "justification": "[ç®€æ´ã€ç²¾ç¡®çš„æ ¸å¿ƒç†ç”±]"
  }}
]
"""

CONSISTENCY_CHECK_PROMPT_TEMPLATE = """
è§’è‰²ï¼šéŸ©ä¸­ç¿»è¯‘å¤å®¡ä¸“å®¶ (V3 - ä»²è£æ¨¡å¼)

èº«ä»½ä¸ä½¿å‘½:
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„éŸ©ä¸­ç¿»è¯‘å¤å®¡ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯è§£å†³ä¸€ä¸ªå…·ä½“çš„ç¿»è¯‘ä¸ä¸€è‡´é—®é¢˜ã€‚

å°è¯´èƒŒæ™¯è®¾å®š:
{novel_background}

ä»»åŠ¡ï¼šç¿»è¯‘ä¸€è‡´æ€§ä»²è£
å¯¹äºåŒä¸€ä¸ªéŸ©è¯­åŸæ–‡â€œ{korean_term}â€ï¼Œç°åœ¨å­˜åœ¨å¤šç§ä¸åŒçš„è¯‘æ³•ã€‚è¯·æ ¹æ®â€œå°è¯´èƒŒæ™¯è®¾å®šâ€å’Œæ¯ä¸ªè¯‘æ³•é™„å¸¦çš„â€œæœ¯è¯­æ‰€åœ¨åŸæ–‡å‚è€ƒâ€ï¼Œåˆ¤æ–­å“ªä¸€ä¸ªè¯‘æ³•æ˜¯æœ€ä½³çš„ã€åº”è¢«ç»Ÿä¸€é‡‡ç”¨çš„è¯‘æ³•ã€‚

å­˜åœ¨å†²çªçš„è¯‘æ³•åˆ—è¡¨:
{conflicts_json}

è¯·ä¸¥æ ¼æŒ‰ç…§æˆ‘ç»™å‡ºçš„ JSON æ ¼å¼è¿”å›ä½ çš„æœ€ç»ˆè£å†³ã€‚

è¾“å‡ºæ ¼å¼ (Output Format):
{{
  "korean_term": "{korean_term}",
  "recommended_translation": "[ä½ è£å®šçš„æœ€ä½³ç»Ÿä¸€è¯‘æ³•]"
}}
"""

FUZZY_CONSISTENCY_PROMPT_TEMPLATE = """
è§’è‰²ï¼šéŸ©ä¸­ç¿»è¯‘å¤å®¡ä¸“å®¶ (V3.2 - å®ä½“å…³è”ä»²è£æ¨¡å¼)

èº«ä»½ä¸ä½¿å‘½:
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„éŸ©ä¸­ç¿»è¯‘å¤å®¡ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯è§£å†³ä¸€ç»„å¯èƒ½ç›¸å…³çš„è§’è‰²åç§°çš„ç¿»è¯‘ä¸€è‡´æ€§é—®é¢˜ã€‚

å°è¯´èƒŒæ™¯è®¾å®š:
{novel_background}

ä»»åŠ¡ï¼šè§’è‰²åå…³è”ä¸€è‡´æ€§ä»²è£
ä»¥ä¸‹æ˜¯ä¸€ç»„å¯èƒ½ç›¸å…³çš„è§’è‰²æœ¯è¯­ï¼ˆä¾‹å¦‚ï¼Œå…¨åä¸ç®€ç§°ï¼‰ã€‚è¯·æ£€æŸ¥å®ƒä»¬çš„è¯‘æ³•æ˜¯å¦ä¿æŒäº†é€»è¾‘ä¸Šçš„ä¸€è‡´æ€§ã€‚
ä¾‹å¦‚ï¼Œâ€œç„åœ¨é›„â€å’Œâ€œåœ¨é›„â€çš„ç¿»è¯‘åº”è¯¥æœ‰å…³è”æ€§ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ€ç»´é“¾è¿›è¡Œåˆ¤æ–­ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æœ¯è¯­æœ€ç»ˆæ¨èè¯‘æ³•çš„ JSON åˆ—è¡¨ï¼š
1.  **è¯†åˆ«æ ¸å¿ƒå®ä½“**: åœ¨æœ¯è¯­ç»„ä¸­ï¼Œè¯†åˆ«å‡ºæ ¸å¿ƒçš„è§’è‰²å®ä½“æ˜¯ä»€ä¹ˆã€‚
2.  **è¯„ä¼°ä¸€è‡´æ€§**: æ£€æŸ¥å½“å‰æ¯ä¸ªæœ¯è¯­çš„è¯‘æ³•æ˜¯å¦éƒ½ä¸è¿™ä¸ªæ ¸å¿ƒå®ä½“ä¿æŒäº†ä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œç®€ç§°çš„ç¿»è¯‘æ˜¯å¦æ˜¯å…¨åç¿»è¯‘çš„ä¸€éƒ¨åˆ†ã€‚
3.  **ç»™å‡ºæœ€ç»ˆæ¨è**: ä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªæœ¯è¯­ï¼Œç»™å‡ºä¸€ä¸ªæœ€ç»ˆçš„ã€ä¿æŒäº†ä¸€è‡´æ€§çš„æ¨èè¯‘æ³•ã€‚å¦‚æœæŸä¸ªæœ¯è¯­çš„å½“å‰è¯‘æ³•å·²ç»æ˜¯æœ€ä½³çš„ï¼Œåˆ™æ¨èè¯‘æ³•ä¸å½“å‰è¯‘æ³•ç›¸åŒã€‚

å­˜åœ¨å…³è”å†²çªçš„æœ¯è¯­ç»„:
{conflicts_json}

æœ€ç»ˆæŒ‡ä»¤ï¼šä½ çš„è¾“å‡ºå¿…é¡»ä¸”åªèƒ½æ˜¯ä¸€ä¸ª JSON åˆ—è¡¨ï¼Œä¸¥ç¦åœ¨ JSON åˆ—è¡¨ä¹‹åé™„åŠ ä»»ä½•å½¢å¼çš„è§£é‡Šã€æ ‡é¢˜æˆ–è¯´æ˜æ–‡å­—ã€‚

è¾“å‡ºæ ¼å¼ (Output Format):
[
  {{
    "korean_term": "[è¾“å…¥æœ¯è¯­1çš„åŸæ–‡]",
    "recommended_translation": "[ä½ ä¸ºæœ¯è¯­1è£å®šçš„æœ€ç»ˆè¯‘æ³•]"
  }},
  {{
    "korean_term": "[è¾“å…¥æœ¯è¯­2çš„åŸæ–‡]",
    "recommended_translation": "[ä½ ä¸ºæœ¯è¯­2è£å®šçš„æœ€ç»ˆè¯‘æ³•]"
  }}
]
"""


# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (Core Functions) ---

def load_config():
    """åŠ è½½æˆ–åˆ›å»º cfg.json é…ç½®æ–‡ä»¶ã€‚"""
    global CONFIG
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        script_dir = os.getcwd()
        
    config_path = os.path.join(script_dir, 'cfg.json')

    if not os.path.exists(config_path):
        print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸ºæ‚¨åˆ›å»ºä¸€ä¸ªæ¨¡æ¿...")
        default_config = {
            "api_key": "åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„APIå¯†é’¥",
            "base_url": "https://api.deepseek.com/v1",
            "model": "deepseek-reasoner",
            "MAX_WORKERS": 10,
            "BATCH_SIZE": 10,
            "default_directory": "/Users/doudouda/Downloads/2/"
        }
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
        print(f"è¯·åœ¨ {config_path} æ–‡ä»¶ä¸­å¡«å…¥æ‚¨çš„ API å¯†é’¥åé‡æ–°è¿è¡Œè„šæœ¬ã€‚")
        return False

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            CONFIG = json.load(f)
        if CONFIG.get("api_key") == "åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„APIå¯†é’¥":
            print(f"é”™è¯¯ï¼šè¯·å…ˆåœ¨ {config_path} æ–‡ä»¶ä¸­å¡«å…¥æ‚¨çš„ API å¯†é’¥ã€‚")
            return False
        return True
    except (json.JSONDecodeError, Exception) as e:
        print(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def find_input_files(directory: str):
    """åœ¨æŒ‡å®šç›®å½•æŸ¥æ‰¾æœ¯è¯­è¡¨ (.xlsx) å’Œå‚è€ƒæ–‡ä»¶ (.txt)ã€‚"""
    global GLOSSARY_FILE_PATH, REFERENCE_FILE_PATH
    excluded_files = [FINAL_GLOSSARY_FILENAME, MODIFICATION_LOG_FILENAME]
    
    xlsx_files = []
    for file in os.listdir(directory):
        if file.endswith('.xlsx') and not file.startswith('~') and file not in excluded_files:
            xlsx_files.append(file)
        elif file.endswith('.txt'):
            REFERENCE_FILE_PATH = os.path.join(directory, file)
    
    if not xlsx_files:
        raise FileNotFoundError(f"é”™è¯¯ï¼šåœ¨ç›®å½• '{directory}' ä¸­æœªæ‰¾åˆ°æº .xlsx æœ¯è¯­è¡¨æ–‡ä»¶ã€‚")
    if len(xlsx_files) > 1:
        print(f"è­¦å‘Šï¼šåœ¨ç›®å½• '{directory}' ä¸­æ‰¾åˆ°å¤šä¸ª .xlsx æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶: {xlsx_files[0]}")
    
    GLOSSARY_FILE_PATH = os.path.join(directory, xlsx_files[0])
    
    if not REFERENCE_FILE_PATH:
        raise FileNotFoundError(f"é”™è¯¯ï¼šåœ¨ç›®å½• '{directory}' ä¸­æœªæ‰¾åˆ° .txt å‚è€ƒæ–‡ä»¶ã€‚")
    
    print(f"æ‰¾åˆ°æœ¯è¯­è¡¨æ–‡ä»¶: {GLOSSARY_FILE_PATH}")
    print(f"æ‰¾åˆ°å‚è€ƒæ–‡ä»¶: {REFERENCE_FILE_PATH}")


def load_data(glossary_path: str, reference_path: str) -> (pd.DataFrame, Dict[str, str], pd.Series, List[str]):
    """ä»æœ¯è¯­è¡¨å’Œå‚è€ƒæ–‡ä»¶ä¸­åŠ è½½æ•°æ®ï¼Œå¹¶ä¿ç•™åŸå§‹æ•°æ®ç±»å‹å’Œåˆ—åã€‚"""
    print("æ­£åœ¨åŠ è½½æ–‡ä»¶...")
    glossary_df_orig = pd.read_excel(glossary_path, engine='openpyxl')
    original_dtypes = glossary_df_orig.dtypes
    original_cols = glossary_df_orig.columns.tolist()

    if len(original_cols) < 2:
        raise ValueError("æœ¯è¯­è¡¨ Excel æ–‡ä»¶å¿…é¡»è‡³å°‘åŒ…å«ä¸¤åˆ—ï¼ˆåŸæ–‡å’Œè¯‘æ–‡ï¼‰ã€‚")
    rename_map = {original_cols[0]: 'src', original_cols[1]: 'dst'}
    glossary_df_renamed = glossary_df_orig.rename(columns=rename_map)
    
    glossary_df = glossary_df_renamed.fillna('').astype(str)
    print(f"æˆåŠŸåŠ è½½æœ¯è¯­è¡¨ï¼Œå…± {len(glossary_df)} æ¡ã€‚")

    with open(reference_path, 'r', encoding='utf-8') as f:
        content = f.read().replace('\r\n', '\n').replace('\r', '\n')
    
    blocks = content.split('åŸæ–‡ï¼š')[1:]
    reference_dict = {}
    for i, block in enumerate(blocks):
        match = re.search(
            r'^(?P<korean_term>.*?)\nè¯‘æ–‡ï¼š.*?\nå¤‡æ³¨ï¼š.*?\nå‡ºç°æ¬¡æ•°ï¼š.*?\nå‚è€ƒæ–‡æœ¬ï¼š.*?\n(?P<context>.*)',
            block,
            re.DOTALL
        )
        if match:
            data = match.groupdict()
            korean_term = data['korean_term'].strip()
            context = data['context'].strip().replace("â€»", "")
            reference_dict[korean_term] = context
        else:
            match_alt = re.search(r'^(?P<korean_term>.*?)\nå‚è€ƒæ–‡æœ¬ï¼š.*\n(?P<context>.*)', block, re.DOTALL)
            if match_alt:
                data = match_alt.groupdict()
                korean_term = data['korean_term'].strip()
                context = data['context'].strip().replace("â€»", "")
                reference_dict[korean_term] = context

    print(f"æˆåŠŸè§£æå‚è€ƒæ–‡ä»¶ï¼Œå…± {len(reference_dict)} ä¸ªæœ¯è¯­çš„ä¸Šä¸‹æ–‡ã€‚")
    return glossary_df, reference_dict, original_dtypes, original_cols


def call_ai_api(client: openai.OpenAI, prompt: str) -> Optional[str]:
    """é€šç”¨ API è°ƒç”¨å‡½æ•°ï¼ŒåŒ…å«è‡ªé€‚åº”èŠ‚æµå’Œé‡è¯•é€»è¾‘ã€‚"""
    global rate_limit_pause_event
    max_retries = 3
    base_retry_delay = 5

    if rate_limit_pause_event.is_set():
        print(f"çº¿ç¨‹ {threading.get_ident()}: æ£€æµ‹åˆ°å…¨å±€æš‚åœï¼Œç­‰å¾…...")
        rate_limit_pause_event.wait()

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=CONFIG.get("model", "deepseek-reasoner"),
                messages=[{"role": "user", "content": prompt}],
                max_tokens=8192,
                temperature=0.1
            )
            return response.choices[0].message.content
        except openai.RateLimitError:
            if not rate_limit_pause_event.is_set():
                print("\næ£€æµ‹åˆ° API é€Ÿç‡é™åˆ¶ï¼è§¦å‘å…¨å±€æš‚åœ...")
                rate_limit_pause_event.set()
            
            wait_time = (base_retry_delay * (2 ** attempt)) + random.uniform(0, 1)
            print(f"çº¿ç¨‹ {threading.get_ident()}: é€Ÿç‡è¶…é™ï¼Œå°†åœ¨ {wait_time:.2f} ç§’åé‡è¯•...")
            time.sleep(wait_time)

            if attempt == max_retries - 1:
                print("æœ€åä¸€ä¸ªé‡è¯•çº¿ç¨‹å®Œæˆç­‰å¾…ï¼Œè§£é™¤å…¨å±€æš‚åœã€‚")
                rate_limit_pause_event.clear()
        except Exception as e:
            if attempt == max_retries - 1:
                log_error(f"APIè¯·æ±‚åœ¨ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥ã€‚é”™è¯¯: {e}\nPrompt: {prompt[:500]}...")
                return None
    
    return None


def parse_ai_json_response(response_text: str) -> Optional[Any]:
    """ä» AI çš„å›å¤ä¸­è§£æ JSON å¯¹è±¡ï¼Œå¢åŠ ä¸‰å±‚é˜²å¾¡å®¹é”™æœºåˆ¶ã€‚"""
    if not response_text: return None
    
    clean_text = re.sub(r'```json\s*|\s*```', '', response_text).strip()
    
    try:
        return json.loads(clean_text)
    except json.JSONDecodeError:
        json_match = re.search(r'(\[.*?\]|\{.*?\})', clean_text, re.DOTALL)
        if json_match:
            extracted_json = json_match.group(1)
            try:
                return json.loads(extracted_json)
            except json.JSONDecodeError:
                pass 
        
        if clean_text.startswith('[') and not clean_text.endswith(']'):
            last_brace_pos = clean_text.rfind('}')
            if last_brace_pos != -1:
                fixed_text = clean_text[:last_brace_pos+1] + ']'
                try:
                    return json.loads(fixed_text)
                except json.JSONDecodeError as e:
                    log_error(f"JSONä¿®å¤åè§£æä»ç„¶å¤±è´¥ã€‚é”™è¯¯: {e}\nä¿®å¤å°è¯•: {fixed_text}")
                    return None
        
        log_error(f"JSONè§£æåœ¨æ‰€æœ‰é˜²å¾¡å±‚å‡å¤±è´¥ã€‚\nåŸå§‹æ–‡æœ¬: {response_text}")
        return None
    except Exception as e:
        log_error(f"è§£æ AI å›å¤æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

def process_batch(args: tuple) -> Optional[List[dict]]:
    """å¤„ç†ä¸€æ‰¹æœ¯è¯­çš„å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹ã€‚"""
    batch_df, novel_background, reference_dict, client = args
    
    batch_list = []
    character_keywords = ['è§’è‰²', 'ç¥ç¥‡/ä¼ è¯´äººç‰©', 'ç”·æ€§è§’è‰²', 'å¥³æ€§è§’è‰²']
    for _, row in batch_df.iterrows():
        korean_term = row['src'].strip()
        batch_list.append({
            "korean_term": korean_term,
            "chinese_translation": row['dst'].strip(),
            "is_character": any(keyword in row.get('info', '') for keyword in character_keywords),
            "context": reference_dict.get(korean_term, f"æœªåœ¨å‚è€ƒæ–‡ä»¶ä¸­æ‰¾åˆ°æœ¯è¯­ '{korean_term}' çš„ä¸Šä¸‹æ–‡ã€‚")
        })
    
    prompt = BATCH_REVIEW_PROMPT_TEMPLATE.format(
        novel_background=novel_background,
        batch_json=json.dumps(batch_list, ensure_ascii=False, indent=2)
    )
    
    ai_response_text = call_ai_api(client, prompt)
    return parse_ai_json_response(ai_response_text)


def get_multiline_input(prompt_message: str) -> str:
    """è·å–ç”¨æˆ·å¤šè¡Œè¾“å…¥ã€‚"""
    print(prompt_message)
    lines = []
    while True:
        try:
            line = input()
            if not line:
                break
            lines.append(line)
        except EOFError:
            break
    return "\n".join(lines)

def log_error(message: str):
    """å°†é”™è¯¯ä¿¡æ¯è®°å½•åˆ° error_log.txtã€‚"""
    with open(ERROR_LOG_OUTPUT_PATH, 'a', encoding='utf-8') as f:
        f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\n\n")

def save_results(final_df, log_df, original_dtypes, original_cols):
    """ä¿å­˜æœ€ç»ˆç»“æœåˆ° Excel æ–‡ä»¶ï¼Œå¹¶æ·»åŠ ç­›é€‰åŠŸèƒ½ã€‚"""
    print("\næ­£åœ¨ä¿å­˜ç»“æœ...")
    
    rename_map_reverse = {'src': original_cols[0], 'dst': original_cols[1]}
    final_df_to_save = final_df.rename(columns=rename_map_reverse)

    # æ¢å¤åŸå§‹æ•°æ®ç±»å‹ï¼ŒåŒæ—¶ä¿®å¤ç©ºåˆ—è¢«å¡«å……ä¸º 0 çš„é—®é¢˜
    for col, dtype in original_dtypes.items():
        if col in final_df_to_save.columns:
            try:
                # æ£€æŸ¥åŸå§‹æ•°æ®ç±»å‹æ˜¯å¦ä¸ºæ•°å­—ï¼ˆæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰
                if 'int' in str(dtype) or 'float' in str(dtype):
                    # å°†åˆ—è½¬æ¢ä¸ºæ•°å­—ï¼Œæ— æ³•è½¬æ¢çš„å€¼ï¼ˆå¦‚ç©ºå­—ç¬¦ä¸²ï¼‰ä¼šå˜æˆ NaN
                    numeric_col = pd.to_numeric(final_df_to_save[col], errors='coerce')
                    
                    # --- BUG FIX START ---
                    # å¦‚æœåŸå§‹ç±»å‹æ˜¯æµ®ç‚¹æ•°ï¼Œåˆ™ä¿ç•™ NaNï¼ˆåœ¨ Excel ä¸­æ˜¾ç¤ºä¸ºç©ºå•å…ƒæ ¼ï¼‰
                    # è¿™å¯ä»¥é˜²æ­¢åƒ 'regex' è¿™æ ·å®Œå…¨ä¸ºç©ºçš„åˆ—è¢«é”™è¯¯åœ°å¡«å……ä¸º 0
                    if 'float' in str(dtype):
                        final_df_to_save[col] = numeric_col
                    # å¦‚æœåŸå§‹ç±»å‹æ˜¯æ•´æ•°ï¼Œåˆ™å°† NaN å¡«å……ä¸º 0 å¹¶è½¬æ¢ä¸ºæ•´æ•°ï¼Œä»¥ä¿æŒåŸæœ‰çš„è¡Œä¸º
                    elif 'int' in str(dtype):
                        final_df_to_save[col] = numeric_col.fillna(0).astype(int)
                    # --- BUG FIX END ---
                else:
                    # å¯¹äºéæ•°å­—ç±»å‹ï¼Œç›´æ¥è½¬æ¢å›åŸå§‹ç±»å‹
                    final_df_to_save[col] = final_df_to_save[col].astype(dtype)
            except (ValueError, TypeError):
                # å¦‚æœç±»å‹è½¬æ¢å¤±è´¥ï¼Œåˆ™å¿½ç•¥å¹¶ä¿æŒåŸæ ·
                pass

    try:
        with pd.ExcelWriter(FINAL_GLOSSARY_OUTPUT_PATH, engine='xlsxwriter') as writer:
            final_df_to_save.to_excel(writer, index=False, sheet_name='Sheet1')
            worksheet = writer.sheets['Sheet1']
            (max_row, max_col) = final_df_to_save.shape
            worksheet.autofilter(0, 0, max_row, max_col - 1)
        print(f"æˆåŠŸä¿å­˜æœ€ç»ˆæœ¯è¯­è¡¨åˆ°: {FINAL_GLOSSARY_OUTPUT_PATH}")
        
        if not log_df.empty:
            if 'count' in log_df.columns:
                log_df['count'] = pd.to_numeric(log_df['count'], errors='coerce').fillna(0).astype(int)

            with pd.ExcelWriter(MODIFICATION_LOG_OUTPUT_PATH, engine='xlsxwriter') as writer:
                log_df.to_excel(writer, index=False, sheet_name='Modifications')
                worksheet = writer.sheets['Modifications']
                (max_row, max_col) = log_df.shape
                worksheet.autofilter(0, 0, max_row, max_col - 1)
            print(f"æˆåŠŸä¿å­˜ä¿®æ”¹æ—¥å¿—åˆ°: {MODIFICATION_LOG_OUTPUT_PATH}")
        else:
            print("æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ä¿®æ”¹ï¼Œæœªç”Ÿæˆä¿®æ”¹æ—¥å¿—æ–‡ä»¶ã€‚")

    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å¤„ç†å‡½æ•°ï¼Œè´Ÿè´£ä¸¤é˜¶æ®µå®¡æŸ¥æµç¨‹ã€‚"""
    if not load_config():
        sys.exit(1)

    default_dir = CONFIG.get("default_directory", "./")
    directory_path = input(f"è¯·è¾“å…¥æ–‡ä»¶æ‰€åœ¨ç›®å½• (é»˜è®¤: {default_dir}): ").strip()
    if not directory_path:
        directory_path = default_dir
    
    novel_background = get_multiline_input("è¯·è¾“å…¥å°è¯´èƒŒæ™¯è®¾å®š (è¾“å…¥ç©ºè¡Œå¹¶å›è½¦ä»¥ç»“æŸ): ")
    if not novel_background:
        novel_background = "æ— ç‰¹å®šèƒŒæ™¯è®¾å®šã€‚"

    global FINAL_GLOSSARY_OUTPUT_PATH, MODIFICATION_LOG_OUTPUT_PATH, ERROR_LOG_OUTPUT_PATH
    FINAL_GLOSSARY_OUTPUT_PATH = os.path.join(directory_path, FINAL_GLOSSARY_FILENAME)
    MODIFICATION_LOG_OUTPUT_PATH = os.path.join(directory_path, MODIFICATION_LOG_FILENAME)
    ERROR_LOG_OUTPUT_PATH = os.path.join(directory_path, ERROR_LOG_FILENAME)

    try:
        find_input_files(directory_path)
        glossary_df, reference_dict, original_dtypes, original_cols = load_data(GLOSSARY_FILE_PATH, REFERENCE_FILE_PATH)
    except (FileNotFoundError, ValueError) as e:
        print(e)
        return

    client = openai.OpenAI(api_key=CONFIG["api_key"], base_url=CONFIG["base_url"])
    
    # --- åˆå§‹åŒ–å˜é‡ ---
    processed_rows = []
    modification_log = []
    final_df = pd.DataFrame()

    # --- æ ¸å¿ƒä¿®æ­£: å°†æ•´ä¸ªå¤„ç†æµç¨‹åŒ…è£¹åœ¨ try...except ä¸­ ---
    try:
        # --- æ–­ç‚¹ç»­ä¼ é€»è¾‘ ---
        if os.path.exists(MODIFICATION_LOG_OUTPUT_PATH):
            print(f"æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ—¥å¿—æ–‡ä»¶ '{MODIFICATION_LOG_OUTPUT_PATH}'ï¼Œå°†åœ¨æ­¤åŸºç¡€ä¸Šç»§ç»­ã€‚")
            log_df_existing = pd.read_excel(MODIFICATION_LOG_OUTPUT_PATH)
            modification_log = log_df_existing.to_dict('records')
            
            processed_src_terms = set(log_df_existing['æœ¯è¯­åŸæ–‡'].unique())
            
            processed_df = glossary_df[glossary_df['src'].isin(processed_src_terms)].copy()
            terms_to_process_df = glossary_df[~glossary_df['src'].isin(processed_src_terms)].copy()
            
            for log_entry in modification_log:
                if log_entry['å®¡æŸ¥é˜¶æ®µ'] == 'é€æ¡å®¡æŸ¥':
                    term_src = log_entry['æœ¯è¯­åŸæ–‡']
                    action = log_entry['æ“ä½œ']
                    if action == 'ä¿®æ”¹':
                        processed_df.loc[processed_df['src'] == term_src, 'dst'] = log_entry['æ–°è¯‘æ–‡']
                    elif action == 'åˆ é™¤':
                        processed_df = processed_df[processed_df['src'] != term_src]

            processed_rows = processed_df.to_dict('records')
            print(f"å·²å¤„ç† {len(processed_src_terms)} ä¸ªæœ¯è¯­ï¼Œå‰©ä½™ {len(terms_to_process_df)} ä¸ªå¾…å¤„ç†ã€‚")
        else:
            terms_to_process_df = glossary_df.copy()

        # --- é˜¶æ®µä¸€ï¼šæ‰¹é‡å®¡æŸ¥ (å¹¶è¡Œå¤„ç†) ---
        if not terms_to_process_df.empty:
            print("\n" + "="*20 + " é˜¶æ®µä¸€ï¼šå¼€å§‹æ‰¹é‡å®¡æŸ¥ " + "="*20)
            
            batch_size = CONFIG.get("BATCH_SIZE", 10)
            batches = [terms_to_process_df.iloc[i:i + batch_size] for i in range(0, len(terms_to_process_df), batch_size)]
            tasks = [(batch, novel_background, reference_dict, client) for batch in batches]
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG.get("MAX_WORKERS", 10)) as executor:
                future_to_batch = {executor.submit(process_batch, task): task for task in tasks}
                for future in tqdm(concurrent.futures.as_completed(future_to_batch), total=len(tasks), desc="é˜¶æ®µä¸€å®¡æŸ¥è¿›åº¦"):
                    batch_results = future.result()
                    original_batch_df = future_to_batch[future][0]

                    if batch_results and len(batch_results) == len(original_batch_df):
                        for i, ai_result in enumerate(batch_results):
                            original_row_dict = original_batch_df.iloc[i].to_dict()
                            row_series = pd.Series(original_row_dict)
                            current_row_for_next_phase = original_row_dict.copy()
                            
                            log_entry_base = {'æœ¯è¯­åŸæ–‡': row_series['src'], 'åŸè¯‘æ–‡': row_series['dst'], 'info': row_series.get('info', ''), 'count': row_series.get('count', '')}

                            if ai_result.get('should_delete'):
                                modification_log.append({**log_entry_base, 'å®¡æŸ¥é˜¶æ®µ': 'é€æ¡å®¡æŸ¥', 'æ“ä½œ': 'åˆ é™¤', 'æ–°è¯‘æ–‡': '', 'åˆ¤æ–­ç»“æœ': ai_result.get('judgment_emoji'), 'åˆ¤æ–­ä¾æ®': f"{ai_result.get('deletion_reason')}: {ai_result.get('justification')}"})
                                continue
                            
                            recommended_dst = ai_result.get('recommended_translation', '').strip()
                            if recommended_dst and recommended_dst != row_series['dst'].strip():
                                modification_log.append({**log_entry_base, 'å®¡æŸ¥é˜¶æ®µ': 'é€æ¡å®¡æŸ¥', 'æ“ä½œ': 'ä¿®æ”¹', 'æ–°è¯‘æ–‡': recommended_dst, 'åˆ¤æ–­ç»“æœ': ai_result.get('judgment_emoji'), 'åˆ¤æ–­ä¾æ®': ai_result.get('justification')})
                                current_row_for_next_phase['dst'] = recommended_dst
                            else:
                                modification_log.append({**log_entry_base, 'å®¡æŸ¥é˜¶æ®µ': 'é€æ¡å®¡æŸ¥', 'æ“ä½œ': 'ä¿ç•™', 'æ–°è¯‘æ–‡': row_series['dst'], 'åˆ¤æ–­ç»“æœ': ai_result.get('judgment_emoji'), 'åˆ¤æ–­ä¾æ®': ai_result.get('justification')})
                            processed_rows.append(current_row_for_next_phase)
                    else:
                        log_error(f"æ‰¹å¤„ç†å¤±è´¥æˆ–è¿”å›ç»“æœæ•°é‡ä¸åŒ¹é…ã€‚æ‰¹æ¬¡åŸæ–‡: {[row['src'] for _, row in original_batch_df.iterrows()]}")
                        for _, row in original_batch_df.iterrows():
                            log_entry_base = {'æœ¯è¯­åŸæ–‡': row['src'], 'åŸè¯‘æ–‡': row['dst'], 'info': row.get('info', ''), 'count': row.get('count', '')}
                            modification_log.append({**log_entry_base, 'å®¡æŸ¥é˜¶æ®µ': 'é€æ¡å®¡æŸ¥', 'æ“ä½œ': 'å¤±è´¥', 'æ–°è¯‘æ–‡': row['dst'], 'åˆ¤æ–­ç»“æœ': 'âŒ', 'åˆ¤æ–­ä¾æ®': 'æ‰¹å¤„ç†å¤±è´¥æˆ–AIè¿”å›æ ¼å¼é”™è¯¯'})
                        processed_rows.extend(original_batch_df.to_dict('records'))

        first_pass_df = pd.DataFrame(processed_rows)

        # --- é˜¶æ®µäºŒï¼šè„šæœ¬é©±åŠ¨çš„ä¸€è‡´æ€§ç»ˆå®¡ ---
        print("\n" + "="*20 + " é˜¶æ®µäºŒï¼šå¼€å§‹ä¸€è‡´æ€§ç»ˆå®¡ " + "="*20)
        
        final_df = first_pass_df.copy()
        
        # 2a: å®Œå…¨åŒ¹é…ä¸€è‡´æ€§æ£€æŸ¥
        duplicates = final_df[final_df.duplicated('src', keep=False)]
        conflicts = duplicates.groupby('src')['dst'].nunique()
        conflict_groups = conflicts[conflicts > 1].index.tolist()

        if conflict_groups:
            print(f"è„šæœ¬å‘ç° {len(conflict_groups)} ç»„ã€Œå®Œå…¨åŒ¹é…ã€ç¿»è¯‘ä¸ä¸€è‡´çš„æœ¯è¯­ï¼Œæ­£åœ¨è¯·æ±‚ AI ä»²è£...")
            for term_src in tqdm(conflict_groups, desc="å®Œå…¨åŒ¹é…ä»²è£è¿›åº¦"):
                conflict_rows = final_df[final_df['src'] == term_src]
                conflicts_list = [{"translation": row['dst'], "context": reference_dict.get(term_src, "æ— ä¸Šä¸‹æ–‡")} for _, row in conflict_rows.iterrows()]
                prompt = CONSISTENCY_CHECK_PROMPT_TEMPLATE.format(novel_background=novel_background, korean_term=term_src, conflicts_json=json.dumps(conflicts_list, ensure_ascii=False, indent=2))
                decision = parse_ai_json_response(call_ai_api(client, prompt))
                if decision and decision.get('recommended_translation'):
                    recommended_trans = decision['recommended_translation']
                    indices_to_update = final_df[final_df['src'] == term_src].index
                    for idx in indices_to_update:
                        original_trans = final_df.loc[idx, 'dst']
                        if original_trans != recommended_trans:
                            modification_log.append({'å®¡æŸ¥é˜¶æ®µ': 'æœ€ç»ˆæ ¡å¯¹', 'æœ¯è¯­åŸæ–‡': term_src, 'åŸè¯‘æ–‡': original_trans, 'info': final_df.loc[idx, 'info'], 'count': final_df.loc[idx, 'count'],'æ“ä½œ': 'ä¿®æ”¹ (ä¸€è‡´æ€§)', 'æ–°è¯‘æ–‡': recommended_trans, 'åˆ¤æ–­ç»“æœ': 'âš ï¸', 'åˆ¤æ–­ä¾æ®': f"ç»Ÿä¸€ä¸ºæ¨èè¯‘æ³• '{recommended_trans}'"})
                            final_df.loc[idx, 'dst'] = recommended_trans
                else:
                    log_error(f"å®Œå…¨åŒ¹é…ä¸€è‡´æ€§ä»²è£å¤±è´¥ï¼Œæœ¯è¯­: {term_src}")

        # 2b: æ¨¡ç³ŠåŒ¹é…ï¼ˆè§’è‰²åï¼‰ä¸€è‡´æ€§æ£€æŸ¥
        character_keywords = ['è§’è‰²', 'ç¥ç¥‡/ä¼ è¯´äººç‰©', 'ç”·æ€§è§’è‰²', 'å¥³æ€§è§’è‰²']
        char_df = final_df[final_df['info'].str.contains('|'.join(character_keywords), na=False)].copy()
        char_df = char_df.sort_values(by='src', key=lambda x: x.str.len(), ascending=False)
        
        processed_chars = set()
        fuzzy_conflict_groups = []
        for _, row in char_df.iterrows():
            full_name = row['src']
            if full_name in processed_chars:
                continue
            
            related_group = [full_name]
            processed_chars.add(full_name)
            
            for _, other_row in char_df.iterrows():
                short_name = other_row['src']
                if short_name != full_name and short_name in full_name and short_name not in processed_chars:
                    related_group.append(short_name)
                    processed_chars.add(short_name)
            
            if len(related_group) > 1:
                fuzzy_conflict_groups.append(related_group)

        if fuzzy_conflict_groups:
            print(f"è„šæœ¬å‘ç° {len(fuzzy_conflict_groups)} ç»„ã€Œæ¨¡ç³Šå…³è”ã€çš„è§’è‰²æœ¯è¯­ï¼Œæ­£åœ¨è¯·æ±‚ AI ä»²è£...")
            for group in tqdm(fuzzy_conflict_groups, desc="æ¨¡ç³Šå…³è”ä»²è£è¿›åº¦"):
                group_df = final_df[final_df['src'].isin(group)]
                conflicts_list = [{"korean_term": row['src'], "current_translation": row['dst']} for _, row in group_df.iterrows()]
                prompt = FUZZY_CONSISTENCY_PROMPT_TEMPLATE.format(novel_background=novel_background, conflicts_json=json.dumps(conflicts_list, ensure_ascii=False, indent=2))
                decisions = parse_ai_json_response(call_ai_api(client, prompt))
                
                if isinstance(decisions, list):
                    for decision in decisions:
                        term_to_update = decision.get('korean_term')
                        recommended_trans = decision.get('recommended_translation')
                        if term_to_update and recommended_trans:
                            indices_to_update = final_df[final_df['src'] == term_to_update].index
                            for idx in indices_to_update:
                                original_trans = final_df.loc[idx, 'dst']
                                if original_trans != recommended_trans:
                                    modification_log.append({'å®¡æŸ¥é˜¶æ®µ': 'æœ€ç»ˆæ ¡å¯¹', 'æœ¯è¯­åŸæ–‡': term_to_update, 'åŸè¯‘æ–‡': original_trans, 'info': final_df.loc[idx, 'info'], 'count': final_df.loc[idx, 'count'], 'æ“ä½œ': 'ä¿®æ”¹ (æ¨¡ç³Šä¸€è‡´æ€§)', 'æ–°è¯‘æ–‡': recommended_trans, 'åˆ¤æ–­ç»“æœ': 'âš ï¸', 'åˆ¤æ–­ä¾æ®': f"ä¸ºä¿æŒå…³è”ä¸€è‡´æ€§ï¼Œç»Ÿä¸€ä¸º '{recommended_trans}'"})
                                    final_df.loc[idx, 'dst'] = recommended_trans
                else:
                    log_error(f"æ¨¡ç³Šå…³è”ä¸€è‡´æ€§ä»²è£å¤±è´¥ï¼Œæœ¯è¯­ç»„: {group}")

        # --- ä¿å­˜ç»“æœ ---
        save_results(final_df, pd.DataFrame(modification_log), original_dtypes, original_cols)
        print("\nå¤„ç†å®Œæˆï¼")

    except KeyboardInterrupt:
        print("\næ•è·åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)ï¼æ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
        # ç¡®ä¿å³ä½¿åœ¨ç¬¬äºŒé˜¶æ®µä¸­æ–­ï¼Œä¹Ÿèƒ½ä¿å­˜ç¬¬ä¸€é˜¶æ®µçš„æˆæœ
        if 'first_pass_df' in locals():
             final_df_to_save = first_pass_df
        else:
             final_df_to_save = pd.DataFrame(processed_rows)
             
        save_results(final_df_to_save, pd.DataFrame(modification_log), original_dtypes, original_cols)
        sys.exit(0)


# --- è„šæœ¬å…¥å£ (Script Entry Point) ---
if __name__ == "__main__":
    check_dependencies()
    main()
